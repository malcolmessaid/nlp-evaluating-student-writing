{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ea68dee-139d-4fa5-8c89-b807fe5424e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "#import t5Utils\n",
    "#from t5Utils import get_target_id_map\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "path = 'feedback-prize-2021'\n",
    "train_csv = os.path.join(path, 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd28f854-2773-44c3-b1ea-4af01842eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data_helper(path, tokenizer, df, train_ids):\n",
    "    training_samples = []\n",
    "    for idx in tqdm(train_ids):\n",
    "        filename = os.path.join(path, \"train\", idx + \".txt\")\n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        input_ids = encoded_text[\"input_ids\"]\n",
    "        input_labels = copy.deepcopy(input_ids)\n",
    "        offset_mapping = encoded_text[\"offset_mapping\"]\n",
    "\n",
    "        for k in range(len(input_labels)):\n",
    "            input_labels[k] = \"O\"\n",
    "\n",
    "        sample = {\n",
    "            \"id\": idx,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"text\": text,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "        }\n",
    "\n",
    "        temp_df = df[df[\"id\"] == idx]\n",
    "        # each row of the df is going to be one sentence with one type\n",
    "        for _, row in temp_df.iterrows():\n",
    "            # id is just one document\n",
    "            text_labels = [0] * len(text)\n",
    "            discourse_start = int(row[\"discourse_start\"])\n",
    "            discourse_end = int(row[\"discourse_end\"])\n",
    "            prediction_label = row[\"discourse_type\"]\n",
    "            text_labels[discourse_start:discourse_end] = [1] * (discourse_end - discourse_start)\n",
    "            target_idx = []\n",
    "            # iterating over the offset mapping for the encoded text (tokenized text) so you are basically iterating over every single word (token)\n",
    "            for map_idx, (offset1, offset2) in enumerate(encoded_text[\"offset_mapping\"]):\n",
    "                if sum(text_labels[offset1:offset2]) > 0:\n",
    "                    if len(text[offset1:offset2].split()) > 0:\n",
    "                        target_idx.append(map_idx)\n",
    "\n",
    "            targets_start = target_idx[0]\n",
    "            targets_end = target_idx[-1]\n",
    "            pred_start = \"B-\" + prediction_label\n",
    "            pred_end = \"I-\" + prediction_label\n",
    "            input_labels[targets_start] = pred_start\n",
    "            input_labels[targets_start + 1 : targets_end + 1] = [pred_end] * (targets_end - targets_start)\n",
    "\n",
    "        sample[\"input_ids\"] = input_ids\n",
    "        sample[\"input_labels\"] = input_labels\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "target_id_map = {\n",
    "        \"B-Lead\": 0,\n",
    "        \"I-Lead\": 1,\n",
    "        \"B-Position\": 2,\n",
    "        \"I-Position\": 3,\n",
    "        \"B-Evidence\": 4,\n",
    "        \"I-Evidence\": 5,\n",
    "        \"B-Claim\": 6,\n",
    "        \"I-Claim\": 7,\n",
    "        \"B-Concluding Statement\": 8,\n",
    "        \"I-Concluding Statement\": 9,\n",
    "        \"B-Counterclaim\": 10,\n",
    "        \"I-Counterclaim\": 11,\n",
    "        \"B-Rebuttal\": 12,\n",
    "        \"I-Rebuttal\": 13,\n",
    "        \"O\": 14,\n",
    "        \"PAD\": -100,\n",
    "    }\n",
    "\n",
    "id_target_map = {v: k for k, v in target_id_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "299f98e7-539d-4df2-a6b5-1811020511cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBALS\n",
    "N_EPOCHS = 3 # starting guess\n",
    "BATCH_SIZE = 4 # also a starting guess \n",
    "NUM_WORKERS = 4 # start at the same as batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3659ce23-57b2-4ff7-ac7e-d0ad1a2fc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, samples, tokenizer, max_len):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #input_ids = self.samples[index]['input_ids']\n",
    "        input_labels = self.samples[index]['input_labels']\n",
    "        # GET FROM ABISHEK THE target_id_map\n",
    "        input_labels = [target_id_map[x] for x in input_labels]\n",
    "        other_label_id = target_id_map[\"O\"]\n",
    "        padding_label_id = target_id_map[\"PAD\"]\n",
    "        \n",
    "        text = self.samples['text']\n",
    "        \n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids = endcoded_text['input_ids']\n",
    "        mask = encoded_text['attention_mask']\n",
    "        token_type_id = encoded_text['token_type_ids']\n",
    "        \n",
    "        \n",
    "        # add start token id to the input_ids\n",
    "        #input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "        input_labels = [other_label_id] + input_labels\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "            input_labels = input_labels[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        #input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        input_labels = input_labels + [other_label_id]\n",
    "        \n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            if self.tokenizer.padding_side == \"right\":\n",
    "                input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "                input_labels = input_labels + [padding_label_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "            else:\n",
    "                input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "                input_labels = [padding_label_id] * padding_length + input_labels\n",
    "                attention_mask = [0] * padding_length + attention_mask\n",
    "        \n",
    "        \n",
    "        \n",
    "        # add start token id \n",
    "        #input_ids = [self.tokenizer.decoder_start_token_ ] + input_ids\n",
    "        \n",
    "        ## !!! add other token to the labels !!!\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'input_labels': torch.tensor(input_labels, dtype=torch.float)\n",
    "        }\n",
    "        return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82e053a7-fbbd-45e9-ac1d-85a3a57af133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_df,\n",
    "                 test_df,\n",
    "                 tokenizer: T5Tokenizer,\n",
    "                 batch_size: int,\n",
    "                 max_len: int):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        return\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = EssayDataset(self.train_df, self.tokenizer, self.max_len)\n",
    "        self.test_dataset = EssayDataset(self.test_df, self.tokenizer, self.max_len)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "371b8956-a438-42eb-b165-8c39da1fc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EssayT5Model(LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)#, return_dict=True)\n",
    "        return\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        ouput = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['mask']\n",
    "        labels = batch['input_labels']\n",
    "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['mask']\n",
    "        labels = batch['input_labels']\n",
    "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['mask']\n",
    "        labels = batch['input_labels']\n",
    "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameter(), lr=0.0001) \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09bee0f8-3ae8-48cd-8c39-28190b57910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/15594 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████| 15594/15594 [02:44<00:00, 94.55it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(train_csv)\n",
    "essay_ids = df['id'].unique()\n",
    "samples = prepare_training_data_helper(path, tokenizer, df, essay_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a759f583-6dbb-4c46-9bf1-b307402946a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(samples)\n",
    "random.seed(42)\n",
    "test_idxs = set(random.sample(range(n), int(n*0.1)))\n",
    "test = [samples[idx] for idx in range(n) if idx in test_idxs]\n",
    "train = [samples[idx] for idx in range(n) if idx not in test_idxs]\n",
    "data_module = EssayDataModule(train, test, tokenizer, batch_size=BATCH_SIZE, max_len=1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfe610-4e80-44f3-9016-7878b1870f9b",
   "metadata": {},
   "source": [
    "# model = EssayT5Model(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2802fe1-06fc-4ff2-9f99-081fb8faf18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c004b-4c17-4560-a4f1-bd1dd96dacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training ang saving chekcpoints, train model.\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger('lightning_logs', name='essay_evaluation')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    gpus=1,\n",
    "    progress_bar_regresh_rate=30\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb91070-9e69-4873-8f58-7185d28389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = EssayT5Mode.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
