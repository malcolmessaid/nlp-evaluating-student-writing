{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea68dee-139d-4fa5-8c89-b807fe5424e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "train_csv = os.path.join('feedback-prize-2021', 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cdd59ab-d16c-4e6e-9603-8b5a575ef651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "299f98e7-539d-4df2-a6b5-1811020511cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBALS\n",
    "N_EPOCHS = 3 #TODO\n",
    "BATCH_SIZE = 0 #TODO\n",
    "NUM_WORKERS = 0 #TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3659ce23-57b2-4ff7-ac7e-d0ad1a2fc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, samples, tokenizer, max_len):\n",
    "        self.samples = samples\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.length = len(samples)\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #input_ids = self.samples[index]['input_ids']\n",
    "        input_labels = self.samples[index]['input_labels']\n",
    "        # GET FROM ABISHEK THE target_id_map\n",
    "        input_labels = [target_id_map[x] for x in input_labels]\n",
    "        other_label_id = target_id_map[\"O\"]\n",
    "        padding_label_id = target_id_map[\"PAD\"]\n",
    "        \n",
    "        text = self.samples['text']\n",
    "        \n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        input_ids = endcoded_text['input_ids']\n",
    "        mask = encoded_text['attention_mask']\n",
    "        token_type_id = encoded_text['token_type_ids']\n",
    "        \n",
    "        \n",
    "        # add start token id to the input_ids\n",
    "        #input_ids = [self.tokenizer.cls_token_id] + input_ids\n",
    "        input_labels = [other_label_id] + input_labels\n",
    "\n",
    "        if len(input_ids) > self.max_len - 1:\n",
    "            input_ids = input_ids[: self.max_len - 1]\n",
    "            input_labels = input_labels[: self.max_len - 1]\n",
    "\n",
    "        # add end token id to the input_ids\n",
    "        #input_ids = input_ids + [self.tokenizer.sep_token_id]\n",
    "        input_labels = input_labels + [other_label_id]\n",
    "        \n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            if self.tokenizer.padding_side == \"right\":\n",
    "                input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "                input_labels = input_labels + [padding_label_id] * padding_length\n",
    "                attention_mask = attention_mask + [0] * padding_length\n",
    "            else:\n",
    "                input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n",
    "                input_labels = [padding_label_id] * padding_length + input_labels\n",
    "                attention_mask = [0] * padding_length + attention_mask\n",
    "        \n",
    "        \n",
    "        \n",
    "        # add start token id \n",
    "        #input_ids = [self.tokenizer.decoder_start_token_ ] + input_ids\n",
    "        \n",
    "        ## !!! add other token to the labels !!!\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'input_labels': torch.tensor(input_labels, dtype=torch.float)\n",
    "        }\n",
    "        return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82e053a7-fbbd-45e9-ac1d-85a3a57af133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 train_df: pd.DataFrame,\n",
    "                 test_df: pd.DataFrame,\n",
    "                 tokenizer: T5Tokenizer,\n",
    "                 batch_size: int,\n",
    "                 max_len: int):\n",
    "        super().__init__()\n",
    "        self.train_df\n",
    "        self.test_df\n",
    "        self.batch_size\n",
    "        self.tokenizer\n",
    "        self.max_len\n",
    "        return\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = EssayDataset(self.train_df, self.tokenizer, self.max_len)\n",
    "        self.test_dataset = EssayDataset(self.test_df, self.tokenizer, self.max_len)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=self.train_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset=self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3790d59-b716-440b-8ef3-d012edb04b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 't5-base'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "data_module = EssayDataModule(train_df, test_df, tokenizer, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "371b8956-a438-42eb-b165-8c39da1fc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayT5Model(LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)#, return_dict=True)\n",
    "        return\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        ouput = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['mask']\n",
    "        labels = batch['input_labels']\n",
    "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['mask']\n",
    "        labels = batch['input_labels']\n",
    "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['mask']\n",
    "        labels = batch['input_labels']\n",
    "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameter(), lr=0.0001) \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "567d6754-a6f1-427c-b030-ebb3ce6a47df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872c136cfb81459dac037035ede04955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEssayT5Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mEssayT5Model.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\u001b[38;5;66;03m#, return_dict=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/UChicago/Year4/CMSC25700/final_project/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1206\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1208\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1209\u001b[0m     modules[name] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "model = EssayT5Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238c004b-4c17-4560-a4f1-bd1dd96dacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    #dirpath='checkpoints',\n",
    "    #filename='best-checkpoint',\n",
    "    #save_top_k=1,\n",
    "    #verbose=True,\n",
    "    #monitor='val_loss',\n",
    "    #mode='min'\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger('lightning_logs', name='essay_evaluation')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    #logger=logger,\n",
    "    #checkpoint_callback=checkpoint_callback,\n",
    "    #max_epochs=N_EPOCHS,\n",
    "    #gpus=1,\n",
    "    #progress_bar_regresh_rate=30\n",
    ")\n",
    "trainer.fit(model, data_module)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb91070-9e69-4873-8f58-7185d28389aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = EssayT5Mode.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
